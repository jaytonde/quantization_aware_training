{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83951a90",
   "metadata": {},
   "source": [
    "### Step 1 : Installations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64da860b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os, re\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    import torch; v = re.match(r\"[0-9\\.]{3,}\", str(torch.__version__)).group(0)\n",
    "    xformers = \"xformers==\" + (\"0.0.32.post2\" if v == \"2.8.0\" else \"0.0.29.post3\")\n",
    "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth\n",
    "!pip install torchao==0.14.0 fbgemm-gpu-genai==1.3.0\n",
    "!pip install transformers==4.55.4\n",
    "!pip install --no-deps trl==0.22.2\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dd33a7",
   "metadata": {},
   "source": [
    "### Step 2 : Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84c2670",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TMA benchmarks will be running without grid constant TMA descriptor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Your Flash Attention 2 installation seems to be broken?\n",
      "A possible explanation is you have a new CUDA version which isn't\n",
      "yet compatible with FA2? Please file a ticket to Unsloth or FA2.\n",
      "We shall now use Xformers instead, which does not have any performance hits!\n",
      "We found this negligible impact by benchmarking on 1x A100.\n",
      "========\n",
      "Switching to PyTorch attention since your Xformers is broken.\n",
      "========\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/flash_attn_2_cuda.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3c104cuda9SetDeviceEi\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import wandb\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from unsloth.chat_templates import standardize_data_formats\n",
    "\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "from transformers import TextStreamer\n",
    "\n",
    "from torchao.quantization import quantize_\n",
    "from torchao.quantization.qat import QATConfig\n",
    "\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "from torchao.quantization import Int4WeightOnlyConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer,TorchAoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c30df8e",
   "metadata": {},
   "source": [
    "### Step 3 : Experiment configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7a8a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjaytonde05\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/usr/local/lib/python3.10/dist-packages/pydantic/main.py:314: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  Expected `list[str]` but got `tuple` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/wandb/run-20251111_211622-qj03sk8d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jaytonde05/QuantizationTraining/runs/qj03sk8d' target=\"_blank\">Qwen3_4B_QAT</a></strong> to <a href='https://wandb.ai/jaytonde05/QuantizationTraining' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jaytonde05/QuantizationTraining' target=\"_blank\">https://wandb.ai/jaytonde05/QuantizationTraining</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jaytonde05/QuantizationTraining/runs/qj03sk8d' target=\"_blank\">https://wandb.ai/jaytonde05/QuantizationTraining/runs/qj03sk8d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Detected [huggingface_hub.inference] in use.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/jaytonde05/QuantizationTraining/runs/qj03sk8d?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fa4a6213700>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QUANTIZATION_TYPE = \"QAT\"          # Options: \"PTQ\" or \"QAT\"\n",
    "\n",
    "WANDB_PROJECT_NAME = \"QuantizationTraining\"\n",
    "RUN_NAME     = \"Qwen3_4B_\" + QUANTIZATION_TYPE\n",
    "wandb.init(project=WANDB_PROJECT_NAME, name=RUN_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39785ea0",
   "metadata": {},
   "source": [
    "### Step 4 : Load the Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98d7d939",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.11.2: Fast Qwen3 patching. Transformers: 4.55.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 9.0. CUDA Toolkit: 12.8. Triton: 3.4.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = None. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45178eed69314ee5b21f43e26bfcb2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-Instruct-2507\",\n",
    "    max_seq_length = 2048,   \n",
    "    load_in_4bit = False,    \n",
    "    load_in_8bit = False,    \n",
    "    full_finetuning = False,\n",
    ")\n",
    "\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen3-instruct\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f2d409e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen3ForCausalLM(\n",
       "  (model): Qwen3Model(\n",
       "    (embed_tokens): Embedding(151936, 2560, padding_idx=151654)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen3DecoderLayer(\n",
       "        (self_attn): Qwen3Attention(\n",
       "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
       "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen3MLP(\n",
       "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
       "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e23bd3",
   "metadata": {},
   "source": [
    "### Step 5 : Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe21ba8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.2 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Applying QAT to mitigate quantization degradation\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, \n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, \n",
    "    bias = \"none\",    \n",
    "    qat_scheme = \"int4\" if QUANTIZATION_TYPE == \"QAT\" else None,\n",
    "    use_gradient_checkpointing = \"unsloth\", \n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  \n",
    "    loftq_config = None, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "334a112b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT is applied!\n"
     ]
    }
   ],
   "source": [
    "for module in model.modules():\n",
    "    if \"FakeQuantized\" in module.__class__.__name__:\n",
    "        print(\"QAT is applied!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362ef515",
   "metadata": {},
   "source": [
    "### Step 6 : Load training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b616846e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mlabonne/FineTome-100k\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e75a6b90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one sample : {'conversations': [{'content': 'What is the modulus operator in programming and how can I use it to calculate the modulus of two given numbers?', 'role': 'user'}, {'content': 'In programming, the modulus operator is represented by the \\'%\\' symbol. It calculates the remainder when one number is divided by another. To calculate the modulus of two given numbers, you can use the modulus operator in the following way:\\n\\n```python\\n# Calculate the modulus\\nModulus = a % b\\n\\nprint(\"Modulus of the given numbers is: \", Modulus)\\n```\\n\\nIn this code snippet, the variables \\'a\\' and \\'b\\' represent the two given numbers for which you want to calculate the modulus. By using the modulus operator \\'%\\', we calculate the remainder when \\'a\\' is divided by \\'b\\'. The result is then stored in the variable \\'Modulus\\'. Finally, the modulus value is printed using the \\'print\\' statement.\\n\\nFor example, if \\'a\\' is 10 and \\'b\\' is 4, the modulus calculation would be 10 % 4, which equals 2. Therefore, the output of the above code would be:\\n\\n```\\nModulus of the given numbers is: 2\\n```\\n\\nThis means that the modulus of 10 and 4 is 2.', 'role': 'assistant'}], 'source': 'infini-instruct-top-500k', 'score': 4.774171352386475}\n"
     ]
    }
   ],
   "source": [
    "dataset = standardize_data_formats(dataset)\n",
    "print(f\"one sample : {dataset[100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacfd53f",
   "metadata": {},
   "source": [
    "### Step 7 : Apply chat tempelate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1d6499f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def formatting_prompts_func(examples):\n",
    "   convos = examples[\"conversations\"]\n",
    "   texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
    "   return { \"text\" : texts, }\n",
    "\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b6c8e",
   "metadata": {},
   "source": [
    "dataset[100]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db576ae8",
   "metadata": {},
   "source": [
    "### Step 9 : Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce3ebcbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    eval_dataset = None, \n",
    "    args = SFTConfig(\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 4,\n",
    "        gradient_accumulation_steps = 4, \n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = 1, \n",
    "        #max_steps = 30,\n",
    "        learning_rate = 2e-5, \n",
    "        logging_steps = 50,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.001,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"wandb\", \n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "042badc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77586b0",
   "metadata": {},
   "source": [
    "tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2772cb1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'                              In programming, the modulus operator is represented by the \\'%\\' symbol. It calculates the remainder when one number is divided by another. To calculate the modulus of two given numbers, you can use the modulus operator in the following way:\\n\\n```python\\n# Calculate the modulus\\nModulus = a % b\\n\\nprint(\"Modulus of the given numbers is: \", Modulus)\\n```\\n\\nIn this code snippet, the variables \\'a\\' and \\'b\\' represent the two given numbers for which you want to calculate the modulus. By using the modulus operator \\'%\\', we calculate the remainder when \\'a\\' is divided by \\'b\\'. The result is then stored in the variable \\'Modulus\\'. Finally, the modulus value is printed using the \\'print\\' statement.\\n\\nFor example, if \\'a\\' is 10 and \\'b\\' is 4, the modulus calculation would be 10 % 4, which equals 2. Therefore, the output of the above code would be:\\n\\n```\\nModulus of the given numbers is: 2\\n```\\n\\nThis means that the modulus of 10 and 4 is 2.<|im_end|>\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535106ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 100,000 | Num Epochs = 1 | Total steps = 6,250\n",
      "O^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n",
      " \"-____-\"     Trainable parameters = 33,030,144 of 4,055,498,240 (0.81% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='299' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 299/6250 12:31 < 4:11:05, 0.40 it/s, Epoch 0.05/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.975500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.711400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.704900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.656800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.679600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81cbebc",
   "metadata": {},
   "source": [
    "### Step 10 : Convert step in QAT / PTQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b375e71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if QUANTIZATION_TYPE == \"PTQ\":\n",
    "\n",
    "    basline_name = RUN_NAME.replace(\"PTQ\", \"baseline\")\n",
    "    merged_model = trainer.model.merge_and_unload()\n",
    "    merged_model.save_pretrained(f\"./{basline_name}\")\n",
    "    tokenizer.save_pretrained(f\"./{basline_name}\")\n",
    "\n",
    "    api = HfApi()\n",
    "\n",
    "    repo_id=f\"jaytonde05/{basline_name}\",\n",
    "\n",
    "    create_repo(\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\",\n",
    "        token=\"\",\n",
    "        exist_ok=True  # Won't raise error if repo already exists\n",
    "    )\n",
    "\n",
    "    api.upload_folder(\n",
    "        folder_path=f\"./{basline_name}\",\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"model\", # specify this is a model repository\n",
    "        token=\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5152f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if QUANTIZATION_TYPE == \"QAT\":\n",
    "    quantize_(model, QATConfig(step = \"convert\"))\n",
    "else:\n",
    "    quant_config = Int4WeightOnlyConfig(group_size=128, int4_packing_format=\"tile_packed_to_4d\", int4_choose_qparams_algorithm=\"hqq\")\n",
    "    quantization_config = TorchAoConfig(quant_type=quant_config)\n",
    "    #model_ptq, tokenizer = FastLanguageModel.from_pretrained(basline_name, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n",
    "    model = AutoModelForCausalLM.from_pretrained(basline_name, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(basline_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc721da",
   "metadata": {},
   "source": [
    "### Step 11 : Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21647516",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : \"Continue the sequence: 1, 1, 2, 3, 5, 8,\"}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "_ = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1000, # Increase for longer outputs!\n",
    "    temperature = 0.7, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc0c28b",
   "metadata": {},
   "source": [
    "### Step 12 : Save the model to upload on huggingface hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4c6b19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if QUANTIZATION_TYPE == \"QAT\":\n",
    "    model.save_pretrained_torchao(\n",
    "        RUN_NAME,\n",
    "        tokenizer,\n",
    "        torchao_config = model._torchao_config.base_config,\n",
    "    )\n",
    "else:\n",
    "    model.save_pretrained(f\"./{RUN_NAME}\")\n",
    "    tokenizer.save_pretrained(f\"./{RUN_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8b3fd",
   "metadata": {},
   "source": [
    "### Step 13 : Pushing the model on HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480b48c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "repo_id=f\"jaytonde05/{RUN_NAME}\"\n",
    "\n",
    "create_repo(\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\",\n",
    "    token=\"\",\n",
    "    exist_ok=True  # Won't raise error if repo already exists\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=RUN_NAME,\n",
    "    repo_id=repo_id,\n",
    "    repo_type=\"model\", \n",
    "    token=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ba4fd2-43fc-4abe-b77e-09586153af79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
